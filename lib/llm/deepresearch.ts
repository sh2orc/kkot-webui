import { BaseLLM } from "./base";
import { LLMMessage, LLMResponse } from "./types";

export interface DeepResearchStep {
  id: string;
  title: string;
  question: string;
  analysis: string;
  sources?: string[];
  confidence: number;
}

export interface DeepResearchResult {
  query: string;
  steps: DeepResearchStep[];
  synthesis: string;
  finalAnswer: string;
  confidence: number;
  methodology: string;
}

export interface DeepResearchConfig {
  maxSteps: number;
  confidenceThreshold: number;
  analysisDepth: 'basic' | 'intermediate' | 'advanced';
  includeSourceCitations: boolean;
  language: 'ko' | 'en';
}

export class DeepResearchProcessor {
  private llm: BaseLLM;
  private config: DeepResearchConfig;

  constructor(llm: BaseLLM, config: Partial<DeepResearchConfig> = {}) {
    this.llm = llm;
    this.config = {
      maxSteps: 5,
      confidenceThreshold: 0.8,
      analysisDepth: 'intermediate',
      includeSourceCitations: false,
      language: 'ko',
      ...config
    };
  }

  /**
   * ë”¥ë¦¬ì„œì¹˜ ìˆ˜í–‰
   */
  async performDeepResearch(
    query: string,
    context: string = "",
    onProgress?: (step: DeepResearchStep) => void,
    onStream?: (content: string, type: 'step' | 'synthesis' | 'final', stepInfo?: { title?: string, isComplete?: boolean }) => void
  ): Promise<DeepResearchResult> {
    try {
      console.log('=== Deep Research Started ===');
      console.log('Query:', query);
      
      // 1. í•˜ìœ„ ì§ˆë¬¸ë“¤ì„ ë¨¼ì € ìƒì„±
      console.log('1. Generating sub-questions...');
      const subQuestions = await this.generateSubQuestions(query, context);
      console.log('Sub-questions generated:', subQuestions);
      
      // ê³„íšëœ ìŠ¤íƒ­ë“¤ì„ ë¯¸ë¦¬ ì „ë‹¬
      const plannedSteps = [
        { title: 'ì§ˆë¬¸ ë¶„ì„', type: 'step' },
        ...subQuestions.map(q => ({ title: `ë¶„ì„: ${q}`, type: 'step' })),
        { title: 'ì¢…í•© ë¶„ì„', type: 'synthesis' },
        { title: 'ìµœì¢… ë‹µë³€', type: 'final' }
      ];

      console.log('2. Sending planned steps:', plannedSteps);
      if (onStream) {
        onStream('ë”¥ë¦¬ì„œì¹˜ ê³„íšì´ ìˆ˜ë¦½ë˜ì—ˆìŠµë‹ˆë‹¤.', 'step', { 
          title: 'ë¶„ì„ ê³„íš ìˆ˜ë¦½', 
          isComplete: true,
          totalSteps: plannedSteps.length,
          plannedSteps: plannedSteps
        } as any);
      }

      // 2. ì´ˆê¸° ì§ˆë¬¸ ë¶„ì„
      console.log('3. Starting initial query analysis...');
      const analysisStep = await this.analyzeQuery(query, context, onStream);
      console.log('Initial analysis completed:', analysisStep.title);

      // 3. ë‹¤ê°ë„ ë¶„ì„ ìˆ˜í–‰ - ê³„íšëœ ì œëª©ê³¼ ì •í™•íˆ ì¼ì¹˜í•˜ë„ë¡ ì „ë‹¬
      const steps = [analysisStep];
      
      console.log('4. Starting sub-question analysis...');
      for (let i = 0; i < Math.min(subQuestions.length, this.config.maxSteps - 1); i++) {
        const subQuestion = subQuestions[i];
        const plannedTitle = `ë¶„ì„: ${subQuestion}`;
        console.log(`4.${i+1}. Analyzing sub-question: ${plannedTitle}`);
        const step = await this.analyzeSubQuestion(subQuestion, context, steps, onStream, plannedTitle);
        steps.push(step);
        console.log(`4.${i+1}. Sub-question analysis completed`);
      }

      // 4. ì¢…í•© ë¶„ì„ ìˆ˜í–‰
      console.log('5. Starting synthesis...');
      const synthesis = await this.synthesizeFindings(query, steps, onStream);
      console.log('Synthesis completed');
      
      // 5. ìµœì¢… ë‹µë³€ ìƒì„±
      console.log('6. Generating final answer...');
      const finalAnswer = await this.generateFinalAnswer(query, steps, synthesis, onStream);
      console.log('Final answer generated');

      console.log('=== Deep Research Completed ===');
      return {
        query,
        steps,
        synthesis,
        finalAnswer,
        confidence: this.calculateOverallConfidence(steps),
        methodology: this.generateMethodologyDescription()
      };

    } catch (error) {
      console.error('Deep research error:', error);
      throw new Error(`ë”¥ë¦¬ì„œì¹˜ ìˆ˜í–‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: ${error instanceof Error ? error.message : String(error)}`);
    }
  }

  /**
   * ì´ˆê¸° ì§ˆë¬¸ ë¶„ì„
   */
  private async analyzeQuery(query: string, context: string, onStream?: (content: string, type: 'step' | 'synthesis' | 'final', stepInfo?: { title?: string, isComplete?: boolean }) => void): Promise<DeepResearchStep> {
    console.log('analyzeQuery started');
    const prompt = this.buildAnalysisPrompt(query, context);
    
    const messages: LLMMessage[] = [
      { role: "system", content: this.getSystemPrompt() },
      { role: "user", content: prompt }
    ];

    console.log('Sending start signal for ì§ˆë¬¸ ë¶„ì„');
    // ìŠ¤íŠ¸ë¦¬ë°ìœ¼ë¡œ ì§„í–‰ ìƒí™© ì „ì†¡ (ìƒíƒœë§Œ)
    if (onStream) {
      onStream('', 'step', { title: 'ì§ˆë¬¸ ë¶„ì„', isComplete: false });
    }

    console.log('Calling LLM for analysis...');
    const response = await this.llm.chat(messages);
    console.log('LLM response received, length:', response.content.length);
    
    console.log('Sending completion signal for ì§ˆë¬¸ ë¶„ì„');
    // ì™„ë£Œ í›„ ê²°ê³¼ ì „ì†¡
    if (onStream) {
      onStream(response.content, 'step', { title: 'ì§ˆë¬¸ ë¶„ì„', isComplete: true });
    }
    
    console.log('analyzeQuery completed');
    return {
      id: `step-analysis-${Date.now()}`,
      title: "ì§ˆë¬¸ ë¶„ì„",
      question: query,
      analysis: response.content,
      confidence: 0.8
    };
  }

  /**
   * í•˜ìœ„ ì§ˆë¬¸ ìƒì„± - ê°œì„ ëœ ì¶”ì¶œ ë¡œì§
   */
  private async generateSubQuestions(query: string, context: string): Promise<string[]> {
    const prompt = `ë‹¤ìŒ ì§ˆë¬¸ì„ ê¹Šì´ ë¶„ì„í•˜ê¸° ìœ„í•´ 3-4ê°œì˜ í•˜ìœ„ ì§ˆë¬¸ì„ ìƒì„±í•´ì£¼ì„¸ìš”:

ì§ˆë¬¸: "${query}"
${context ? `ë§¥ë½: ${context}` : ''}

ê° í•˜ìœ„ ì§ˆë¬¸ì€ ë‹¤ìŒê³¼ ê°™ì€ ê´€ì ì—ì„œ ì ‘ê·¼í•´ì•¼ í•©ë‹ˆë‹¤:
- í•µì‹¬ ê°œë… ë¶„ì„
- ë‹¤ì–‘í•œ ê´€ì  íƒìƒ‰
- êµ¬ì²´ì  ì‚¬ë¡€ ì¡°ì‚¬
- ì˜í–¥ ë° ê²°ê³¼ ë¶„ì„

í•˜ìœ„ ì§ˆë¬¸ë“¤ì„ ë‹¤ìŒ í˜•ì‹ìœ¼ë¡œ ì •í™•íˆ ë‚˜ì—´í•´ì£¼ì„¸ìš”:
1. [ì§ˆë¬¸ ë‚´ìš©]
2. [ì§ˆë¬¸ ë‚´ìš©]
3. [ì§ˆë¬¸ ë‚´ìš©]
4. [ì§ˆë¬¸ ë‚´ìš©]`;

    const messages: LLMMessage[] = [
      { role: "system", content: this.getSystemPrompt() },
      { role: "user", content: prompt }
    ];

    const response = await this.llm.chat(messages);
    
    // ì‘ë‹µì—ì„œ ì§ˆë¬¸ë“¤ ì¶”ì¶œ - ë” ê²¬ê³ í•œ ë¡œì§
    const questions = response.content
      .split('\n')
      .filter(line => line.trim().length > 0)
      .map(line => line.trim())
      .filter(line => /^\d+[.)]\s*/.test(line)) // ìˆ«ì + ì  ë˜ëŠ” ê´„í˜¸ íŒ¨í„´
      .map(line => line.replace(/^\d+[.)]\s*/, '').trim()) // ë²ˆí˜¸ ì œê±°
      .filter(q => q.length > 0 && q.length < 200); // ë¹ˆ ë¬¸ìì—´ ë° ë„ˆë¬´ ê¸´ ë¬¸ìì—´ ì œê±°

    console.log('Generated sub-questions:', questions);
    return questions.slice(0, 4); // ìµœëŒ€ 4ê°œ ì§ˆë¬¸
  }

  /**
   * í•˜ìœ„ ì§ˆë¬¸ ë¶„ì„ - ì •í™•í•œ ì œëª© ë§¤ì¹­
   */
  private async analyzeSubQuestion(
    question: string, 
    context: string, 
    previousSteps: DeepResearchStep[],
    onStream?: (content: string, type: 'step' | 'synthesis' | 'final', stepInfo?: { title?: string, isComplete?: boolean }) => void,
    plannedTitle?: string
  ): Promise<DeepResearchStep> {
    const previousContext = previousSteps
      .map(step => `${step.title}: ${step.analysis}`)
      .join('\n\n');

    const prompt = `ë‹¤ìŒ í•˜ìœ„ ì§ˆë¬¸ì„ ë¶„ì„í•´ì£¼ì„¸ìš”:

ì§ˆë¬¸: "${question}"
${context ? `ì›ë³¸ ë§¥ë½: ${context}` : ''}

ì´ì „ ë¶„ì„ ê²°ê³¼:
${previousContext}

ì´ ì§ˆë¬¸ì— ëŒ€í•´ ì²´ê³„ì ì´ê³  ê¹Šì´ ìˆëŠ” ë¶„ì„ì„ ìˆ˜í–‰í•˜ì„¸ìš”.`;

    const messages: LLMMessage[] = [
      { role: "system", content: this.getSystemPrompt() },
      { role: "user", content: prompt }
    ];

    // ê³„íšëœ ì œëª©ì´ ìˆìœ¼ë©´ ì‚¬ìš©, ì—†ìœ¼ë©´ ê¸°ë³¸ ì œëª© ì‚¬ìš©
    const stepTitle = plannedTitle || `ë¶„ì„: ${question}`;

    // ìŠ¤íŠ¸ë¦¬ë°ìœ¼ë¡œ ì§„í–‰ ìƒí™© ì „ì†¡ (ìƒíƒœë§Œ)
    if (onStream) {
      onStream('', 'step', { title: stepTitle, isComplete: false });
    }

    const response = await this.llm.chat(messages);
    
    // ì™„ë£Œ í›„ ê²°ê³¼ ì „ì†¡
    if (onStream) {
      onStream(response.content, 'step', { title: stepTitle, isComplete: true });
    }
    
    return {
      id: `step-${Date.now()}`,
      title: stepTitle,
      question,
      analysis: response.content,
      confidence: 0.75
    };
  }

  /**
   * ì¢…í•© ë¶„ì„ ìˆ˜í–‰
   */
  private async synthesizeFindings(query: string, steps: DeepResearchStep[], onStream?: (content: string, type: 'step' | 'synthesis' | 'final', stepInfo?: { title?: string, isComplete?: boolean }) => void): Promise<string> {
    const findings = steps
      .map(step => `## ${step.title}\n${step.analysis}`)
      .join('\n\n');

    const prompt = `ë‹¤ìŒì€ "${query}"ì— ëŒ€í•œ ë‹¤ê°ë„ ë¶„ì„ ê²°ê³¼ì…ë‹ˆë‹¤:

${findings}

ìœ„ ë¶„ì„ë“¤ì„ ì¢…í•©í•˜ì—¬ í•µì‹¬ ì¸ì‚¬ì´íŠ¸ì™€ íŒ¨í„´ì„ ë„ì¶œí•´ì£¼ì„¸ìš”. 
ë‹¤ìŒ ê´€ì ì—ì„œ ì¢…í•© ë¶„ì„ì„ ìˆ˜í–‰í•˜ì„¸ìš”:
- ê³µí†µ ì£¼ì œì™€ íŒ¨í„´ ì‹ë³„
- ìƒì¶©ë˜ëŠ” ê´€ì ë“¤ì˜ ì¡°í™”
- í•µì‹¬ ì¸ì‚¬ì´íŠ¸ ë„ì¶œ
- ë¯¸í•´ê²° ì§ˆë¬¸ì´ë‚˜ ì¶”ê°€ ì—°êµ¬ í•„ìš” ì˜ì—­ ì‹ë³„`;

    const messages: LLMMessage[] = [
      { role: "system", content: this.getSystemPrompt() },
      { role: "user", content: prompt }
    ];

    // ìŠ¤íŠ¸ë¦¬ë°ìœ¼ë¡œ ì§„í–‰ ìƒí™© ì „ì†¡ (ìƒíƒœë§Œ)
    if (onStream) {
      onStream('', 'synthesis', { title: 'ì¢…í•© ë¶„ì„', isComplete: false });
    }

    const response = await this.llm.chat(messages);
    
    // ì™„ë£Œ í›„ ê²°ê³¼ ì „ì†¡
    if (onStream) {
      onStream(response.content, 'synthesis', { title: 'ì¢…í•© ë¶„ì„', isComplete: true });
    }
    
    return response.content;
  }

  /**
   * ìµœì¢… ë‹µë³€ ìƒì„±
   */
  private async generateFinalAnswer(
    query: string, 
    steps: DeepResearchStep[], 
    synthesis: string,
    onStream?: (content: string, type: 'step' | 'synthesis' | 'final', stepInfo?: { title?: string, isComplete?: boolean }) => void
  ): Promise<string> {
    const prompt = `ì›ë˜ ì§ˆë¬¸: "${query}"

ìˆ˜í–‰í•œ ë¶„ì„ë“¤:
${steps.map(step => `- ${step.title}: ${step.analysis.substring(0, 200)}...`).join('\n')}

ì¢…í•© ë¶„ì„:
${synthesis}

ìœ„ ëª¨ë“  ë¶„ì„ì„ ë°”íƒ•ìœ¼ë¡œ ì›ë˜ ì§ˆë¬¸ì— ëŒ€í•œ í¬ê´„ì ì´ê³  ê¹Šì´ ìˆëŠ” ìµœì¢… ë‹µë³€ì„ ì œê³µí•´ì£¼ì„¸ìš”.
ë‹µë³€ì€ ë‹¤ìŒê³¼ ê°™ì€ êµ¬ì¡°ë¡œ ì‘ì„±í•˜ì„¸ìš”:

1. í•µì‹¬ ë‹µë³€ (ëª…í™•í•˜ê³  ì§ì ‘ì ì¸ ë‹µë³€)
2. ìƒì„¸ ë¶„ì„ (ì£¼ìš” ê´€ì ë“¤ê³¼ ê·¼ê±°)
3. ê³ ë ¤ì‚¬í•­ (ì œí•œì‚¬í•­, ë‹¤ì–‘í•œ ê´€ì )
4. ê²°ë¡  (ìš”ì•½ ë° í–¥í›„ ë°©í–¥)`;

    const messages: LLMMessage[] = [
      { role: "system", content: this.getSystemPrompt() },
      { role: "user", content: prompt }
    ];

    // ìŠ¤íŠ¸ë¦¬ë°ìœ¼ë¡œ ì§„í–‰ ìƒí™© ì „ì†¡ (ìƒíƒœë§Œ)
    if (onStream) {
      onStream('', 'final', { title: 'ìµœì¢… ë‹µë³€', isComplete: false });
    }

    const response = await this.llm.chat(messages);
    
    // ì™„ë£Œ í›„ ê²°ê³¼ ì „ì†¡
    if (onStream) {
      onStream(response.content, 'final', { title: 'ìµœì¢… ë‹µë³€', isComplete: true });
    }
    
    return response.content;
  }

  /**
   * ì „ì²´ ì‹ ë¢°ë„ ê³„ì‚°
   */
  private calculateOverallConfidence(steps: DeepResearchStep[]): number {
    if (steps.length === 0) return 0;
    
    const totalConfidence = steps.reduce((sum, step) => sum + step.confidence, 0);
    return totalConfidence / steps.length;
  }

  /**
   * ë°©ë²•ë¡  ì„¤ëª… ìƒì„±
   */
  private generateMethodologyDescription(): string {
    const depth = this.config.analysisDepth;
    const stepCount = this.config.maxSteps;
    
    return `ë”¥ë¦¬ì„œì¹˜ ë°©ë²•ë¡ : ${depth === 'basic' ? 'ê¸°ë³¸' : depth === 'intermediate' ? 'ì¤‘ê¸‰' : 'ê³ ê¸‰'} ë¶„ì„ (${stepCount}ë‹¨ê³„)`;
  }

  /**
   * ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ ìƒì„±
   */
  private getSystemPrompt(): string {
    return `ë‹¹ì‹ ì€ ì „ë¬¸ ì—°êµ¬ìì´ì ë¶„ì„ê°€ì…ë‹ˆë‹¤. ì£¼ì–´ì§„ ì§ˆë¬¸ì— ëŒ€í•´ ê¹Šì´ ìˆê³  ì²´ê³„ì ì¸ ë¶„ì„ì„ ìˆ˜í–‰í•´ì•¼ í•©ë‹ˆë‹¤.

ë¶„ì„ ì§€ì¹¨:
1. ê°ê´€ì ì´ê³  ê· í˜• ì¡íŒ ê´€ì ì„ ìœ ì§€í•˜ì„¸ìš”
2. ë‹¤ì–‘í•œ ê°ë„ì—ì„œ ì£¼ì œë¥¼ íƒêµ¬í•˜ì„¸ìš”
3. êµ¬ì²´ì ì¸ ì˜ˆì‹œì™€ ê·¼ê±°ë¥¼ ì œì‹œí•˜ì„¸ìš”
4. ë…¼ë¦¬ì ì´ê³  ì²´ê³„ì ì¸ êµ¬ì¡°ë¡œ ë‹µë³€í•˜ì„¸ìš”
5. ë¶ˆí™•ì‹¤í•œ ë¶€ë¶„ì€ ëª…í™•íˆ í‘œì‹œí•˜ì„¸ìš”
6. í•œêµ­ì–´ë¡œ ë‹µë³€í•˜ë˜, ì „ë¬¸ ìš©ì–´ëŠ” ì ì ˆíˆ ì‚¬ìš©í•˜ì„¸ìš”

ë¶„ì„ ê¹Šì´: ${this.config.analysisDepth}
ì–¸ì–´: ${this.config.language === 'ko' ? 'í•œêµ­ì–´' : 'English'}`;
  }

  /**
   * ë¶„ì„ í”„ë¡¬í”„íŠ¸ ìƒì„±
   */
  private buildAnalysisPrompt(query: string, context: string): string {
    return `ë‹¤ìŒ ì§ˆë¬¸ì„ ê¹Šì´ ë¶„ì„í•˜ê³  ì²´ê³„ì ì¸ ì ‘ê·¼ë²•ì„ ìˆ˜ë¦½í•´ì£¼ì„¸ìš”:

ì§ˆë¬¸: "${query}"
${context ? `ë§¥ë½: ${context}` : ''}

ë¶„ì„ ìš”ì†Œ:
1. ì§ˆë¬¸ì˜ í•µì‹¬ ê°œë… ì •ì˜
2. ë¶„ì„í•´ì•¼ í•  ì£¼ìš” ê´€ì ë“¤
3. ê³ ë ¤í•´ì•¼ í•  ìš”ì†Œë“¤
4. ì ‘ê·¼ ë°©ë²•ë¡ 
5. ì˜ˆìƒë˜ëŠ” ë³µì¡ì„±ê³¼ ë‚œì´ë„

ì²´ê³„ì ì´ê³  ê¹Šì´ ìˆëŠ” ë¶„ì„ì„ ìˆ˜í–‰í•˜ì„¸ìš”.`;
  }
}

/**
 * ë”¥ë¦¬ì„œì¹˜ ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜ë“¤
 */
export class DeepResearchUtils {
  /**
   * ë”¥ë¦¬ì„œì¹˜ ê²°ê³¼ë¥¼ ë§ˆí¬ë‹¤ìš´ í˜•ì‹ìœ¼ë¡œ ë³€í™˜
   */
  static formatResultAsMarkdown(result: DeepResearchResult): string {
    let markdown = `# ë”¥ë¦¬ì„œì¹˜ ê²°ê³¼: ${result.query}\n\n`;
    
    markdown += `## ğŸ“Š ì—°êµ¬ ê°œìš”\n`;
    markdown += `- **ì‹ ë¢°ë„**: ${(result.confidence * 100).toFixed(1)}%\n`;
    markdown += `- **ë°©ë²•ë¡ **: ${result.methodology}\n`;
    markdown += `- **ë¶„ì„ ë‹¨ê³„**: ${result.steps.length}ë‹¨ê³„\n\n`;

    markdown += `## ğŸ” ë¶„ì„ ê³¼ì •\n\n`;
    result.steps.forEach((step, index) => {
      markdown += `### ${index + 1}. ${step.title}\n`;
      markdown += `**ì§ˆë¬¸**: ${step.question}\n\n`;
      markdown += `${step.analysis}\n\n`;
      markdown += `**ì‹ ë¢°ë„**: ${(step.confidence * 100).toFixed(1)}%\n\n`;
    });

    markdown += `## ğŸ¯ ì¢…í•© ë¶„ì„\n\n`;
    markdown += `${result.synthesis}\n\n`;

    markdown += `## ğŸ’¡ ìµœì¢… ë‹µë³€\n\n`;
    markdown += `${result.finalAnswer}\n\n`;

    return markdown;
  }

  /**
   * ë”¥ë¦¬ì„œì¹˜ ê²°ê³¼ë¥¼ ìš”ì•½ í˜•ì‹ìœ¼ë¡œ ë³€í™˜
   */
  static formatResultAsSummary(result: DeepResearchResult): string {
    return `**ì§ˆë¬¸**: ${result.query}\n\n${result.finalAnswer}\n\n*${result.steps.length}ë‹¨ê³„ ë”¥ë¦¬ì„œì¹˜ ìˆ˜í–‰ (ì‹ ë¢°ë„: ${(result.confidence * 100).toFixed(1)}%)*`;
  }
}

export default DeepResearchProcessor;
